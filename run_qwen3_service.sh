#!/bin/bash

# æ£€æŸ¥ç«¯å£å‚æ•°
if [ $# -eq 1 ]; then
    CUSTOM_PORT=$1
    # éªŒè¯ç«¯å£å·æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°å­—
    if ! [[ "$CUSTOM_PORT" =~ ^[0-9]+$ ]] || [ "$CUSTOM_PORT" -lt 1024 ] || [ "$CUSTOM_PORT" -gt 65535 ]; then
        echo "âŒ é”™è¯¯: ç«¯å£å·å¿…é¡»æ˜¯ 1024-65535 ä¹‹é—´çš„æ•°å­—"
        echo "ğŸ’¡ ä½¿ç”¨æ–¹æ³•: $0 [ç«¯å£å·]"
        echo "ğŸ“‹ ç¤ºä¾‹: $0 8010"
        exit 1
    fi
    export PORT=$CUSTOM_PORT
else
    export PORT=8010  # é»˜è®¤ç«¯å£
fi

# Set environment variables
export MODEL_PATH="/iflytek/models/qwen/Qwen3-32B"
export HOST="0.0.0.0"
export TENSOR_PARALLEL_SIZE=2  # Adjust based on your GPU count
export GPU_MEMORY_UTILIZATION=0.85  # Optimized for Qwen3-32B
export MAX_MODEL_LEN=32768  # Native context length for Qwen3-32B (can extend to 131072 with YaRN)
export SERVED_MODEL_NAME="Qwen3-32B"  # Clean model name for API calls

# Set OpenBLAS environment variables to fix the threading issue
export OPENBLAS_NUM_THREADS=1
export OMP_NUM_THREADS=1
export NUMEXPR_NUM_THREADS=1
export MKL_NUM_THREADS=1

# Get the current directory for log file
CURRENT_DIR=$(pwd)
LOG_FILE="$CURRENT_DIR/qwen3.log"
CONTAINER_NAME="qwen3-32b-service-${PORT}"

echo "ğŸš€ Starting Qwen3-32B service..."
echo "ğŸ“„ Log file: $LOG_FILE"
echo "ğŸ“ Server will be available at: http://$HOST:$PORT"
echo "ğŸ·ï¸  Model name: $SERVED_MODEL_NAME"
echo "ğŸ”§ Container name: $CONTAINER_NAME"
echo ""

# Remove existing container if it exists
docker rm -f "$CONTAINER_NAME" >/dev/null 2>&1

# Clear previous log file
> "$LOG_FILE"

# Run the Docker container with optimized parameters for Qwen3-32B in background
# Use --gpus '"device=5,6"' to specify GPU IDs 5 and 6
echo "ğŸ³ Starting Docker container..."
docker run -d --name "$CONTAINER_NAME" --gpus '"device=5,6"' \
  --privileged \
  --shm-size=4g \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  -v /iflytek/models:/models \
  -e MODEL=/models/qwen/Qwen3-32B \
  -e HOST=$HOST \
  -e PORT=$PORT \
  -e TENSOR_PARALLEL_SIZE=$TENSOR_PARALLEL_SIZE \
  -e GPU_MEMORY_UTILIZATION=$GPU_MEMORY_UTILIZATION \
  -e CUDA_VISIBLE_DEVICES=5,6 \
  -e OPENBLAS_NUM_THREADS=1 \
  -e OMP_NUM_THREADS=1 \
  -e NUMEXPR_NUM_THREADS=1 \
  -e MKL_NUM_THREADS=1 \
  -p $PORT:$PORT \
  vllm-openai:v0.8.5.post1 \
  --model /models/qwen/Qwen3-32B \
  --served-model-name $SERVED_MODEL_NAME \
  --host $HOST \
  --port $PORT \
  --tensor-parallel-size $TENSOR_PARALLEL_SIZE \
  --gpu-memory-utilization $GPU_MEMORY_UTILIZATION \
  --max-model-len $MAX_MODEL_LEN \
  --enable-reasoning \
  --reasoning-parser deepseek_r1 \
  --max-num-seqs 128 \
  --max-num-batched-tokens 16384

# Check if container started successfully
if [ $? -eq 0 ]; then
    echo "âœ… Docker container started successfully!"
    echo ""
    echo "ğŸ“‹ Service Information:"
    echo "   ğŸ“ URL: http://$HOST:$PORT"
    echo "   ğŸ·ï¸  Model: $SERVED_MODEL_NAME"
    echo "   ğŸ“ Context: $MAX_MODEL_LEN tokens"
    echo "   ğŸ§  Reasoning: enabled (deepseek_r1)"
    echo "   ğŸ”§ Container: $CONTAINER_NAME"
    echo ""
    echo "ğŸ“„ Logs are being written to: $LOG_FILE"
    echo ""
    echo "ğŸ” Showing startup logs (Press Ctrl+C to exit log view, service will continue running):"
    echo "=================================================================================="
    
    # Setup trap to handle Ctrl+C gracefully
    trap "echo -e \"\n\nğŸšª Exiting log view...\"; echo \"âœ… Service is still running in background\"; echo \"ğŸ“„ To view logs: tail -f $LOG_FILE\"; echo \"ğŸ›‘ To stop service: docker stop $CONTAINER_NAME\"; echo \"ğŸ“Š To check status: docker ps | grep $CONTAINER_NAME\"; exit 0" INT
    
    # Follow logs and save to file simultaneously
    docker logs -f "$CONTAINER_NAME" 2>&1 | tee -a "$LOG_FILE"
    
else
    echo "âŒ Failed to start Docker container"
    exit 1
fi
